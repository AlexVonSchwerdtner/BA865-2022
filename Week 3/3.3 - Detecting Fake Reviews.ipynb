{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3.4 - Detecting Fake Reviews.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMhZBgke2P87pKlhNR7GSfH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"o7R5wj0jhcY1","colab":{"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7CgpmdW5jdGlvbiBfdXBsb2FkRmlsZXMoaW5wdXRJZCwgb3V0cHV0SWQpIHsKICBjb25zdCBzdGVwcyA9IHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCk7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICAvLyBDYWNoZSBzdGVwcyBvbiB0aGUgb3V0cHV0RWxlbWVudCB0byBtYWtlIGl0IGF2YWlsYWJsZSBmb3IgdGhlIG5leHQgY2FsbAogIC8vIHRvIHVwbG9hZEZpbGVzQ29udGludWUgZnJvbSBQeXRob24uCiAgb3V0cHV0RWxlbWVudC5zdGVwcyA9IHN0ZXBzOwoKICByZXR1cm4gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpOwp9CgovLyBUaGlzIGlzIHJvdWdobHkgYW4gYXN5bmMgZ2VuZXJhdG9yIChub3Qgc3VwcG9ydGVkIGluIHRoZSBicm93c2VyIHlldCksCi8vIHdoZXJlIHRoZXJlIGFyZSBtdWx0aXBsZSBhc3luY2hyb25vdXMgc3RlcHMgYW5kIHRoZSBQeXRob24gc2lkZSBpcyBnb2luZwovLyB0byBwb2xsIGZvciBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcC4KLy8gVGhpcyB1c2VzIGEgUHJvbWlzZSB0byBibG9jayB0aGUgcHl0aG9uIHNpZGUgb24gY29tcGxldGlvbiBvZiBlYWNoIHN0ZXAsCi8vIHRoZW4gcGFzc2VzIHRoZSByZXN1bHQgb2YgdGhlIHByZXZpb3VzIHN0ZXAgYXMgdGhlIGlucHV0IHRvIHRoZSBuZXh0IHN0ZXAuCmZ1bmN0aW9uIF91cGxvYWRGaWxlc0NvbnRpbnVlKG91dHB1dElkKSB7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICBjb25zdCBzdGVwcyA9IG91dHB1dEVsZW1lbnQuc3RlcHM7CgogIGNvbnN0IG5leHQgPSBzdGVwcy5uZXh0KG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSk7CiAgcmV0dXJuIFByb21pc2UucmVzb2x2ZShuZXh0LnZhbHVlLnByb21pc2UpLnRoZW4oKHZhbHVlKSA9PiB7CiAgICAvLyBDYWNoZSB0aGUgbGFzdCBwcm9taXNlIHZhbHVlIHRvIG1ha2UgaXQgYXZhaWxhYmxlIHRvIHRoZSBuZXh0CiAgICAvLyBzdGVwIG9mIHRoZSBnZW5lcmF0b3IuCiAgICBvdXRwdXRFbGVtZW50Lmxhc3RQcm9taXNlVmFsdWUgPSB2YWx1ZTsKICAgIHJldHVybiBuZXh0LnZhbHVlLnJlc3BvbnNlOwogIH0pOwp9CgovKioKICogR2VuZXJhdG9yIGZ1bmN0aW9uIHdoaWNoIGlzIGNhbGxlZCBiZXR3ZWVuIGVhY2ggYXN5bmMgc3RlcCBvZiB0aGUgdXBsb2FkCiAqIHByb2Nlc3MuCiAqIEBwYXJhbSB7c3RyaW5nfSBpbnB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIGlucHV0IGZpbGUgcGlja2VyIGVsZW1lbnQuCiAqIEBwYXJhbSB7c3RyaW5nfSBvdXRwdXRJZCBFbGVtZW50IElEIG9mIHRoZSBvdXRwdXQgZGlzcGxheS4KICogQHJldHVybiB7IUl0ZXJhYmxlPCFPYmplY3Q+fSBJdGVyYWJsZSBvZiBuZXh0IHN0ZXBzLgogKi8KZnVuY3Rpb24qIHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IGlucHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKGlucHV0SWQpOwogIGlucHV0RWxlbWVudC5kaXNhYmxlZCA9IGZhbHNlOwoKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIG91dHB1dEVsZW1lbnQuaW5uZXJIVE1MID0gJyc7CgogIGNvbnN0IHBpY2tlZFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgaW5wdXRFbGVtZW50LmFkZEV2ZW50TGlzdGVuZXIoJ2NoYW5nZScsIChlKSA9PiB7CiAgICAgIHJlc29sdmUoZS50YXJnZXQuZmlsZXMpOwogICAgfSk7CiAgfSk7CgogIGNvbnN0IGNhbmNlbCA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2J1dHRvbicpOwogIGlucHV0RWxlbWVudC5wYXJlbnRFbGVtZW50LmFwcGVuZENoaWxkKGNhbmNlbCk7CiAgY2FuY2VsLnRleHRDb250ZW50ID0gJ0NhbmNlbCB1cGxvYWQnOwogIGNvbnN0IGNhbmNlbFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgY2FuY2VsLm9uY2xpY2sgPSAoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9OwogIH0pOwoKICAvLyBXYWl0IGZvciB0aGUgdXNlciB0byBwaWNrIHRoZSBmaWxlcy4KICBjb25zdCBmaWxlcyA9IHlpZWxkIHsKICAgIHByb21pc2U6IFByb21pc2UucmFjZShbcGlja2VkUHJvbWlzZSwgY2FuY2VsUHJvbWlzZV0pLAogICAgcmVzcG9uc2U6IHsKICAgICAgYWN0aW9uOiAnc3RhcnRpbmcnLAogICAgfQogIH07CgogIGNhbmNlbC5yZW1vdmUoKTsKCiAgLy8gRGlzYWJsZSB0aGUgaW5wdXQgZWxlbWVudCBzaW5jZSBmdXJ0aGVyIHBpY2tzIGFyZSBub3QgYWxsb3dlZC4KICBpbnB1dEVsZW1lbnQuZGlzYWJsZWQgPSB0cnVlOwoKICBpZiAoIWZpbGVzKSB7CiAgICByZXR1cm4gewogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgICAgfQogICAgfTsKICB9CgogIGZvciAoY29uc3QgZmlsZSBvZiBmaWxlcykgewogICAgY29uc3QgbGkgPSBkb2N1bWVudC5jcmVhdGVFbGVtZW50KCdsaScpOwogICAgbGkuYXBwZW5kKHNwYW4oZmlsZS5uYW1lLCB7Zm9udFdlaWdodDogJ2JvbGQnfSkpOwogICAgbGkuYXBwZW5kKHNwYW4oCiAgICAgICAgYCgke2ZpbGUudHlwZSB8fCAnbi9hJ30pIC0gJHtmaWxlLnNpemV9IGJ5dGVzLCBgICsKICAgICAgICBgbGFzdCBtb2RpZmllZDogJHsKICAgICAgICAgICAgZmlsZS5sYXN0TW9kaWZpZWREYXRlID8gZmlsZS5sYXN0TW9kaWZpZWREYXRlLnRvTG9jYWxlRGF0ZVN0cmluZygpIDoKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgJ24vYSd9IC0gYCkpOwogICAgY29uc3QgcGVyY2VudCA9IHNwYW4oJzAlIGRvbmUnKTsKICAgIGxpLmFwcGVuZENoaWxkKHBlcmNlbnQpOwoKICAgIG91dHB1dEVsZW1lbnQuYXBwZW5kQ2hpbGQobGkpOwoKICAgIGNvbnN0IGZpbGVEYXRhUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICAgIGNvbnN0IHJlYWRlciA9IG5ldyBGaWxlUmVhZGVyKCk7CiAgICAgIHJlYWRlci5vbmxvYWQgPSAoZSkgPT4gewogICAgICAgIHJlc29sdmUoZS50YXJnZXQucmVzdWx0KTsKICAgICAgfTsKICAgICAgcmVhZGVyLnJlYWRBc0FycmF5QnVmZmVyKGZpbGUpOwogICAgfSk7CiAgICAvLyBXYWl0IGZvciB0aGUgZGF0YSB0byBiZSByZWFkeS4KICAgIGxldCBmaWxlRGF0YSA9IHlpZWxkIHsKICAgICAgcHJvbWlzZTogZmlsZURhdGFQcm9taXNlLAogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbnRpbnVlJywKICAgICAgfQogICAgfTsKCiAgICAvLyBVc2UgYSBjaHVua2VkIHNlbmRpbmcgdG8gYXZvaWQgbWVzc2FnZSBzaXplIGxpbWl0cy4gU2VlIGIvNjIxMTU2NjAuCiAgICBsZXQgcG9zaXRpb24gPSAwOwogICAgZG8gewogICAgICBjb25zdCBsZW5ndGggPSBNYXRoLm1pbihmaWxlRGF0YS5ieXRlTGVuZ3RoIC0gcG9zaXRpb24sIE1BWF9QQVlMT0FEX1NJWkUpOwogICAgICBjb25zdCBjaHVuayA9IG5ldyBVaW50OEFycmF5KGZpbGVEYXRhLCBwb3NpdGlvbiwgbGVuZ3RoKTsKICAgICAgcG9zaXRpb24gKz0gbGVuZ3RoOwoKICAgICAgY29uc3QgYmFzZTY0ID0gYnRvYShTdHJpbmcuZnJvbUNoYXJDb2RlLmFwcGx5KG51bGwsIGNodW5rKSk7CiAgICAgIHlpZWxkIHsKICAgICAgICByZXNwb25zZTogewogICAgICAgICAgYWN0aW9uOiAnYXBwZW5kJywKICAgICAgICAgIGZpbGU6IGZpbGUubmFtZSwKICAgICAgICAgIGRhdGE6IGJhc2U2NCwKICAgICAgICB9LAogICAgICB9OwoKICAgICAgbGV0IHBlcmNlbnREb25lID0gZmlsZURhdGEuYnl0ZUxlbmd0aCA9PT0gMCA/CiAgICAgICAgICAxMDAgOgogICAgICAgICAgTWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCk7CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPSBgJHtwZXJjZW50RG9uZX0lIGRvbmVgOwoKICAgIH0gd2hpbGUgKHBvc2l0aW9uIDwgZmlsZURhdGEuYnl0ZUxlbmd0aCk7CiAgfQoKICAvLyBBbGwgZG9uZS4KICB5aWVsZCB7CiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICB9CiAgfTsKfQoKc2NvcGUuZ29vZ2xlID0gc2NvcGUuZ29vZ2xlIHx8IHt9OwpzY29wZS5nb29nbGUuY29sYWIgPSBzY29wZS5nb29nbGUuY29sYWIgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYi5fZmlsZXMgPSB7CiAgX3VwbG9hZEZpbGVzLAogIF91cGxvYWRGaWxlc0NvbnRpbnVlLAp9Owp9KShzZWxmKTsK","ok":true,"headers":[["content-type","application/javascript"]],"status":200,"status_text":""}},"base_uri":"https://localhost:8080/","height":231},"executionInfo":{"status":"ok","timestamp":1643395314110,"user_tz":300,"elapsed":34733,"user":{"displayName":"Gordon Burtch","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi6kdrNKuddVmCp6HcajLgk8KM0o5MC7oJKYfMbVGU=s64","userId":"10144756805379529333"}},"outputId":"31153a75-f7cf-4a92-e3d4-47a4bde5a06a"},"outputs":[{"output_type":"display_data","data":{"text/html":["\n","     <input type=\"file\" id=\"files-c7a47ecd-afcc-4a04-8672-cda34b25c7d4\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-c7a47ecd-afcc-4a04-8672-cda34b25c7d4\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving deceptive-opinion.csv to deceptive-opinion.csv\n"]},{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-2e38610b-0e2c-46bb-a2ca-210e2fdfe3a9\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>deceptive</th>\n","      <th>hotel</th>\n","      <th>polarity</th>\n","      <th>source</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>1600</td>\n","      <td>1600</td>\n","      <td>1600</td>\n","      <td>1600</td>\n","      <td>1600</td>\n","    </tr>\n","    <tr>\n","      <th>unique</th>\n","      <td>2</td>\n","      <td>20</td>\n","      <td>2</td>\n","      <td>3</td>\n","      <td>1596</td>\n","    </tr>\n","    <tr>\n","      <th>top</th>\n","      <td>truthful</td>\n","      <td>sofitel</td>\n","      <td>negative</td>\n","      <td>MTurk</td>\n","      <td>My daughter and I woke in the morning wanting ...</td>\n","    </tr>\n","    <tr>\n","      <th>freq</th>\n","      <td>800</td>\n","      <td>80</td>\n","      <td>800</td>\n","      <td>800</td>\n","      <td>2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2e38610b-0e2c-46bb-a2ca-210e2fdfe3a9')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-2e38610b-0e2c-46bb-a2ca-210e2fdfe3a9 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-2e38610b-0e2c-46bb-a2ca-210e2fdfe3a9');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["       deceptive  ...                                               text\n","count       1600  ...                                               1600\n","unique         2  ...                                               1596\n","top     truthful  ...  My daughter and I woke in the morning wanting ...\n","freq         800  ...                                                  2\n","\n","[4 rows x 5 columns]"]},"metadata":{},"execution_count":1}],"source":["import tensorflow as tf\n","from tensorflow import keras\n","from google.colab import files\n","import pandas as pd\n","import io\n","\n","uploaded = files.upload()\n","trip_advisor = pd.read_csv(io.BytesIO(uploaded['deceptive-opinion.csv']))\n","trip_advisor.describe()"]},{"cell_type":"code","source":["import numpy as np\n","from keras.preprocessing.text import text_to_word_sequence\n","\n","# The dataset is perfectly balanced, so 50% accuracy will be equivalent to a random guess. \n","labels = np.where(trip_advisor['deceptive']=='truthful',0,1)\n","\n","text = []\n","for i in range(len(trip_advisor)):\n","  text.append(text_to_word_sequence(trip_advisor['text'][i])) # This strips punctuation, odd characters, and makes things lower-case. "],"metadata":{"id":"_8dsXMrbk445","executionInfo":{"status":"ok","timestamp":1643395317366,"user_tz":300,"elapsed":107,"user":{"displayName":"Gordon Burtch","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi6kdrNKuddVmCp6HcajLgk8KM0o5MC7oJKYfMbVGU=s64","userId":"10144756805379529333"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["Now, let's strip out infrequent terms."],"metadata":{"id":"PFNRsZqu3JNl"}},{"cell_type":"code","source":["# Let's make a running total of each term's freuqency in the data.\n","word_freq = {}\n","for review in text:\n","  for term in review:\n","    try:\n","        word_freq[term] = word_freq[term]+1\n","    except KeyError:\n","        word_freq[term] = 1\n","\n","# This is where we can play with how frequent a word has to be to enter our model.\n","min_freq = 1\n","max_freq = max(i for i in word_freq.values())\n","for i in range(len(text)):\n","  text[i] = {term for term in text[i] if word_freq[term] >= min_freq & word_freq[term] <= max_freq}\n","\n","unique_terms = {term for review in text for term in review}\n","print(f'We have {len(unique_terms)} unique tokens in our dataset.')\n","\n","# Assign an integer to each unique term.\n","word_index = {term: number for number, term in enumerate(unique_terms)}\n","reverse_index = {number: term for number, term in enumerate(unique_terms)}"],"metadata":{"id":"kySkp5Gi2_yv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1643395353160,"user_tz":300,"elapsed":976,"user":{"displayName":"Gordon Burtch","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi6kdrNKuddVmCp6HcajLgk8KM0o5MC7oJKYfMbVGU=s64","userId":"10144756805379529333"}},"outputId":"15871d02-e126-49b7-ccd2-dc8f9801195d"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["We have 10275 unique tokens in our dataset.\n"]}]},{"cell_type":"markdown","source":["One-hot encoding the text."],"metadata":{"id":"mvIDdBCaiY1f"}},{"cell_type":"code","source":["def vectorize_sequences(sequences, dimension=len(unique_terms)): \n","    # Make our blank matrix of 0's to store hot encodings.\n","    results = np.zeros((len(sequences), dimension))\n","\n","    # For each observation and element in that observation,\n","    # Update the blank matrix to a 1 at row obs, column element value.\n","    for i, sequence in enumerate(sequences):\n","        for j in sequence:\n","            results[i, word_index[j]] = 1.\n","    return results\n","\n","# I am converting the resulting giant arrays into float datatype. They are 'float64' by default, which takes up a ton of RAM!\n","text_onehot = vectorize_sequences(text).astype('float')\n","\n","#print(train_onehot[0,0:34])\n","#print(reverse_index[33])\n","text_onehot.shape"],"metadata":{"id":"NdGsdLNxqyyf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1643395356458,"user_tz":300,"elapsed":333,"user":{"displayName":"Gordon Burtch","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi6kdrNKuddVmCp6HcajLgk8KM0o5MC7oJKYfMbVGU=s64","userId":"10144756805379529333"}},"outputId":"ee7034a4-b83e-4104-b59c-7d983c45add8"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1600, 10275)"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","source":["Don't forget we have a few other features in the data. It's not just about the review text... "],"metadata":{"id":"4MONCGalmvsS"}},{"cell_type":"code","source":["# One hot encoding the hotels. \n","hotel_dict = {hotel : index for index, hotel in enumerate(set(trip_advisor['hotel']))}\n","hotels = []\n","for hotel in trip_advisor['hotel']:\n","  hotels.append(hotel_dict[hotel])\n","\n","hotels_onehot = keras.utils.to_categorical(np.array(hotels))\n","\n","# One hot encoding the review source\n","source_int = np.where(np.array(trip_advisor['source'])=='MTurk',0,np.where(np.array(trip_advisor['source'])=='TripAdvisor',1,2))\n","source_onehot = keras.utils.to_categorical(source_int)\n","\n","# Binarizing the polarity\n","polarity_bin = np.where(np.array(trip_advisor['polarity'])==\"negative\",0,1).reshape(1600,1)\n","\n","# Last step, we shuffle the data\n","data_onehot = np.concatenate((labels.reshape(1600,1),text_onehot,hotels_onehot,polarity_bin),axis=1)\n","np.random.shuffle(data_onehot)\n","\n","# Then we pull out predictors and labels.\n","predictors = data_onehot[:,1:]\n","labels = data_onehot[:,0]\n"],"metadata":{"id":"Jr3WnmXhmzcc","executionInfo":{"status":"ok","timestamp":1643395361228,"user_tz":300,"elapsed":638,"user":{"displayName":"Gordon Burtch","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi6kdrNKuddVmCp6HcajLgk8KM0o5MC7oJKYfMbVGU=s64","userId":"10144756805379529333"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["Now we can fit out model to the resulting data. Once again we have k-fold cross validation here. This model is incapable of learning anything much (validation accuracy never really surpasses 55-56%, and the loss is always increasing in training)."],"metadata":{"id":"pR9UI8U4zkjm"}},{"cell_type":"code","source":["from tensorflow.keras import layers\n","import matplotlib.pyplot as plt\n","\n","def build_model():\n","    model = keras.Sequential([\n","        # This is essentially a dense layer that mimics word embedding; we are reducing the one-hot encoded text (10,000+ one-hot tokens) down to 750 latent dimensions.\n","        layers.Dense(750, activation=\"linear\"),\n","        layers.Dense(50, activation=\"relu\",kernel_regularizer=\"l2\"),\n","        layers.Dense(5, activation=\"relu\"),\n","        layers.Dense(1, activation=\"sigmoid\")\n","    ])\n","    model.compile(optimizer=keras.optimizers.Adadelta(learning_rate=0.01), loss=\"binary_crossentropy\", metrics=[keras.metrics.BinaryAccuracy(threshold=0.5)])\n","    return model\n","\n","model = build_model()\n","\n","data_train = predictors[:1200]\n","labels_train = labels[:1200]\n","data_test = predictors[1200:]\n","labels_test = labels[1200:]\n","\n","k = 4\n","num_validation_samples = len(data_train) // k\n","num_epochs = 50\n","batch_sizes = 25\n","all_loss_histories = []\n","all_val_loss_histories = []  \n","all_acc_histories = []\n","all_val_acc_histories = []\n"," \n","for fold in range(k):\n","    validation_data = data_train[num_validation_samples * fold:\n","                           num_validation_samples * (fold + 1)]\n","    validation_targets = labels_train[num_validation_samples * fold:\n","                           num_validation_samples * (fold + 1)]\n","    training_data = np.concatenate([\n","        data_train[:num_validation_samples * fold],\n","        data_train[num_validation_samples * (fold + 1):]])\n","    training_targets = np.concatenate([\n","        labels_train[:num_validation_samples * fold],\n","        labels_train[num_validation_samples * (fold + 1):]])\n","    model = build_model()\n","    history = model.fit(training_data, training_targets, \n","                        validation_data = (validation_data,validation_targets), \n","                        epochs=num_epochs, batch_size=batch_sizes)\n","    val_loss_history = history.history['val_loss']\n","    val_acc_history = history.history['val_binary_accuracy']\n","    loss_history = history.history['loss']\n","    acc_history = history.history['binary_accuracy']\n","    all_val_loss_histories.append(val_loss_history)\n","    all_loss_histories.append(loss_history)\n","    all_val_acc_histories.append(val_acc_history)\n","    all_acc_histories.append(acc_history)\n","\n","average_loss_history = [np.mean([x[i] for x in all_loss_histories]) for i in range(num_epochs)]\n","average_val_loss_history = [np.mean([x[i] for x in all_val_loss_histories]) for i in range(num_epochs)]\n","average_acc_history = [np.mean([x[i] for x in all_acc_histories]) for i in range(num_epochs)]\n","average_val_acc_history = [np.mean([x[i] for x in all_val_acc_histories]) for i in range(num_epochs)]"],"metadata":{"id":"8sEQSBHNiGSo","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c60ebfec-0042-4b41-9cb0-2788f0b7e72f","executionInfo":{"status":"ok","timestamp":1643397852950,"user_tz":300,"elapsed":150602,"user":{"displayName":"Gordon Burtch","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi6kdrNKuddVmCp6HcajLgk8KM0o5MC7oJKYfMbVGU=s64","userId":"10144756805379529333"}}},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n","36/36 [==============================] - 1s 16ms/step - loss: 1.6303 - binary_accuracy: 0.5133 - val_loss: 1.6206 - val_binary_accuracy: 0.5533\n","Epoch 2/50\n","36/36 [==============================] - 0s 11ms/step - loss: 1.6058 - binary_accuracy: 0.5578 - val_loss: 1.6080 - val_binary_accuracy: 0.5533\n","Epoch 3/50\n","36/36 [==============================] - 0s 11ms/step - loss: 1.5837 - binary_accuracy: 0.5856 - val_loss: 1.5956 - val_binary_accuracy: 0.5533\n","Epoch 4/50\n","36/36 [==============================] - 0s 12ms/step - loss: 1.5611 - binary_accuracy: 0.5922 - val_loss: 1.5822 - val_binary_accuracy: 0.5700\n","Epoch 5/50\n","36/36 [==============================] - 0s 11ms/step - loss: 1.5382 - binary_accuracy: 0.6111 - val_loss: 1.5673 - val_binary_accuracy: 0.6067\n","Epoch 6/50\n","36/36 [==============================] - 0s 11ms/step - loss: 1.5154 - binary_accuracy: 0.6589 - val_loss: 1.5566 - val_binary_accuracy: 0.5800\n","Epoch 7/50\n","36/36 [==============================] - 0s 12ms/step - loss: 1.4942 - binary_accuracy: 0.6711 - val_loss: 1.5433 - val_binary_accuracy: 0.5967\n","Epoch 8/50\n","36/36 [==============================] - 0s 11ms/step - loss: 1.4731 - binary_accuracy: 0.6867 - val_loss: 1.5284 - val_binary_accuracy: 0.6233\n","Epoch 9/50\n","36/36 [==============================] - 0s 12ms/step - loss: 1.4522 - binary_accuracy: 0.7200 - val_loss: 1.5148 - val_binary_accuracy: 0.6567\n","Epoch 10/50\n","36/36 [==============================] - 0s 11ms/step - loss: 1.4322 - binary_accuracy: 0.7500 - val_loss: 1.5030 - val_binary_accuracy: 0.6567\n","Epoch 11/50\n","36/36 [==============================] - 0s 11ms/step - loss: 1.4125 - binary_accuracy: 0.7678 - val_loss: 1.4916 - val_binary_accuracy: 0.6667\n","Epoch 12/50\n","36/36 [==============================] - 0s 11ms/step - loss: 1.3931 - binary_accuracy: 0.7867 - val_loss: 1.4791 - val_binary_accuracy: 0.6700\n","Epoch 13/50\n","36/36 [==============================] - 0s 12ms/step - loss: 1.3742 - binary_accuracy: 0.8156 - val_loss: 1.4681 - val_binary_accuracy: 0.6767\n","Epoch 14/50\n","36/36 [==============================] - 0s 12ms/step - loss: 1.3554 - binary_accuracy: 0.8311 - val_loss: 1.4553 - val_binary_accuracy: 0.6767\n","Epoch 15/50\n","36/36 [==============================] - 0s 12ms/step - loss: 1.3366 - binary_accuracy: 0.8467 - val_loss: 1.4454 - val_binary_accuracy: 0.6800\n","Epoch 16/50\n","36/36 [==============================] - 0s 11ms/step - loss: 1.3184 - binary_accuracy: 0.8567 - val_loss: 1.4302 - val_binary_accuracy: 0.6933\n","Epoch 17/50\n","36/36 [==============================] - 0s 11ms/step - loss: 1.2999 - binary_accuracy: 0.8767 - val_loss: 1.4175 - val_binary_accuracy: 0.7100\n","Epoch 18/50\n","36/36 [==============================] - 0s 11ms/step - loss: 1.2815 - binary_accuracy: 0.8856 - val_loss: 1.4025 - val_binary_accuracy: 0.7233\n","Epoch 19/50\n","36/36 [==============================] - 0s 11ms/step - loss: 1.2635 - binary_accuracy: 0.9089 - val_loss: 1.3921 - val_binary_accuracy: 0.7200\n","Epoch 20/50\n","36/36 [==============================] - 0s 10ms/step - loss: 1.2454 - binary_accuracy: 0.9144 - val_loss: 1.3842 - val_binary_accuracy: 0.7167\n","Epoch 21/50\n","36/36 [==============================] - 0s 11ms/step - loss: 1.2277 - binary_accuracy: 0.9222 - val_loss: 1.3646 - val_binary_accuracy: 0.7467\n","Epoch 22/50\n","36/36 [==============================] - 0s 11ms/step - loss: 1.2100 - binary_accuracy: 0.9311 - val_loss: 1.3557 - val_binary_accuracy: 0.7367\n","Epoch 23/50\n","36/36 [==============================] - 0s 11ms/step - loss: 1.1922 - binary_accuracy: 0.9389 - val_loss: 1.3426 - val_binary_accuracy: 0.7433\n","Epoch 24/50\n","36/36 [==============================] - 0s 11ms/step - loss: 1.1747 - binary_accuracy: 0.9422 - val_loss: 1.3288 - val_binary_accuracy: 0.7467\n","Epoch 25/50\n","36/36 [==============================] - 0s 10ms/step - loss: 1.1571 - binary_accuracy: 0.9489 - val_loss: 1.3127 - val_binary_accuracy: 0.7633\n","Epoch 26/50\n","36/36 [==============================] - 0s 11ms/step - loss: 1.1400 - binary_accuracy: 0.9533 - val_loss: 1.3045 - val_binary_accuracy: 0.7533\n","Epoch 27/50\n","36/36 [==============================] - 0s 11ms/step - loss: 1.1228 - binary_accuracy: 0.9567 - val_loss: 1.2962 - val_binary_accuracy: 0.7500\n","Epoch 28/50\n","36/36 [==============================] - 0s 11ms/step - loss: 1.1056 - binary_accuracy: 0.9589 - val_loss: 1.2838 - val_binary_accuracy: 0.7633\n","Epoch 29/50\n","36/36 [==============================] - 0s 10ms/step - loss: 1.0894 - binary_accuracy: 0.9611 - val_loss: 1.2670 - val_binary_accuracy: 0.7767\n","Epoch 30/50\n","36/36 [==============================] - 0s 12ms/step - loss: 1.0727 - binary_accuracy: 0.9644 - val_loss: 1.2596 - val_binary_accuracy: 0.7633\n","Epoch 31/50\n","36/36 [==============================] - 0s 11ms/step - loss: 1.0567 - binary_accuracy: 0.9689 - val_loss: 1.2403 - val_binary_accuracy: 0.7833\n","Epoch 32/50\n","36/36 [==============================] - 0s 11ms/step - loss: 1.0407 - binary_accuracy: 0.9744 - val_loss: 1.2283 - val_binary_accuracy: 0.7867\n","Epoch 33/50\n","36/36 [==============================] - 0s 12ms/step - loss: 1.0251 - binary_accuracy: 0.9756 - val_loss: 1.2209 - val_binary_accuracy: 0.7867\n","Epoch 34/50\n","36/36 [==============================] - 0s 10ms/step - loss: 1.0097 - binary_accuracy: 0.9778 - val_loss: 1.2098 - val_binary_accuracy: 0.7867\n","Epoch 35/50\n","36/36 [==============================] - 0s 11ms/step - loss: 0.9947 - binary_accuracy: 0.9800 - val_loss: 1.1938 - val_binary_accuracy: 0.7967\n","Epoch 36/50\n","36/36 [==============================] - 0s 11ms/step - loss: 0.9801 - binary_accuracy: 0.9800 - val_loss: 1.1845 - val_binary_accuracy: 0.8000\n","Epoch 37/50\n","36/36 [==============================] - 0s 10ms/step - loss: 0.9656 - binary_accuracy: 0.9800 - val_loss: 1.1750 - val_binary_accuracy: 0.8000\n","Epoch 38/50\n","36/36 [==============================] - 0s 11ms/step - loss: 0.9516 - binary_accuracy: 0.9811 - val_loss: 1.1659 - val_binary_accuracy: 0.8000\n","Epoch 39/50\n","36/36 [==============================] - 0s 10ms/step - loss: 0.9381 - binary_accuracy: 0.9811 - val_loss: 1.1511 - val_binary_accuracy: 0.8133\n","Epoch 40/50\n","36/36 [==============================] - 0s 12ms/step - loss: 0.9249 - binary_accuracy: 0.9811 - val_loss: 1.1449 - val_binary_accuracy: 0.8100\n","Epoch 41/50\n","36/36 [==============================] - 0s 11ms/step - loss: 0.9118 - binary_accuracy: 0.9833 - val_loss: 1.1419 - val_binary_accuracy: 0.8000\n","Epoch 42/50\n","36/36 [==============================] - 0s 11ms/step - loss: 0.8996 - binary_accuracy: 0.9833 - val_loss: 1.1237 - val_binary_accuracy: 0.8167\n","Epoch 43/50\n","36/36 [==============================] - 0s 11ms/step - loss: 0.8872 - binary_accuracy: 0.9833 - val_loss: 1.1101 - val_binary_accuracy: 0.8333\n","Epoch 44/50\n","36/36 [==============================] - 0s 11ms/step - loss: 0.8753 - binary_accuracy: 0.9856 - val_loss: 1.1035 - val_binary_accuracy: 0.8267\n","Epoch 45/50\n","36/36 [==============================] - 0s 12ms/step - loss: 0.8638 - binary_accuracy: 0.9867 - val_loss: 1.1029 - val_binary_accuracy: 0.8200\n","Epoch 46/50\n","36/36 [==============================] - 0s 11ms/step - loss: 0.8526 - binary_accuracy: 0.9867 - val_loss: 1.0876 - val_binary_accuracy: 0.8267\n","Epoch 47/50\n","36/36 [==============================] - 0s 10ms/step - loss: 0.8416 - binary_accuracy: 0.9867 - val_loss: 1.0822 - val_binary_accuracy: 0.8233\n","Epoch 48/50\n","36/36 [==============================] - 0s 11ms/step - loss: 0.8311 - binary_accuracy: 0.9878 - val_loss: 1.0722 - val_binary_accuracy: 0.8300\n","Epoch 49/50\n","36/36 [==============================] - 0s 11ms/step - loss: 0.8204 - binary_accuracy: 0.9878 - val_loss: 1.0614 - val_binary_accuracy: 0.8333\n","Epoch 50/50\n","36/36 [==============================] - 0s 12ms/step - loss: 0.8107 - binary_accuracy: 0.9878 - val_loss: 1.0614 - val_binary_accuracy: 0.8267\n","Epoch 1/50\n","36/36 [==============================] - 1s 17ms/step - loss: 1.6307 - binary_accuracy: 0.5144 - val_loss: 1.6275 - val_binary_accuracy: 0.4833\n","Epoch 2/50\n","36/36 [==============================] - 0s 11ms/step - loss: 1.6090 - binary_accuracy: 0.5178 - val_loss: 1.6120 - val_binary_accuracy: 0.4867\n","Epoch 3/50\n","36/36 [==============================] - 0s 11ms/step - loss: 1.5884 - binary_accuracy: 0.5189 - val_loss: 1.5963 - val_binary_accuracy: 0.4933\n","Epoch 4/50\n","36/36 [==============================] - 0s 12ms/step - loss: 1.5672 - binary_accuracy: 0.5222 - val_loss: 1.5804 - val_binary_accuracy: 0.4967\n","Epoch 5/50\n","36/36 [==============================] - 0s 11ms/step - loss: 1.5464 - binary_accuracy: 0.5289 - val_loss: 1.5653 - val_binary_accuracy: 0.4967\n","Epoch 6/50\n","36/36 [==============================] - 0s 12ms/step - loss: 1.5260 - binary_accuracy: 0.5411 - val_loss: 1.5508 - val_binary_accuracy: 0.5000\n","Epoch 7/50\n","36/36 [==============================] - 0s 12ms/step - loss: 1.5059 - binary_accuracy: 0.5456 - val_loss: 1.5353 - val_binary_accuracy: 0.5067\n","Epoch 8/50\n","36/36 [==============================] - 0s 12ms/step - loss: 1.4866 - binary_accuracy: 0.5656 - val_loss: 1.5229 - val_binary_accuracy: 0.5100\n","Epoch 9/50\n","36/36 [==============================] - 0s 12ms/step - loss: 1.4681 - binary_accuracy: 0.5656 - val_loss: 1.5083 - val_binary_accuracy: 0.5167\n","Epoch 10/50\n","36/36 [==============================] - 0s 11ms/step - loss: 1.4502 - binary_accuracy: 0.5844 - val_loss: 1.4951 - val_binary_accuracy: 0.5167\n","Epoch 11/50\n","36/36 [==============================] - 0s 12ms/step - loss: 1.4331 - binary_accuracy: 0.5956 - val_loss: 1.4818 - val_binary_accuracy: 0.5200\n","Epoch 12/50\n","36/36 [==============================] - 0s 12ms/step - loss: 1.4164 - binary_accuracy: 0.6200 - val_loss: 1.4711 - val_binary_accuracy: 0.5200\n","Epoch 13/50\n","36/36 [==============================] - 0s 14ms/step - loss: 1.4001 - binary_accuracy: 0.6256 - val_loss: 1.4588 - val_binary_accuracy: 0.5200\n","Epoch 14/50\n","36/36 [==============================] - 0s 12ms/step - loss: 1.3846 - binary_accuracy: 0.6400 - val_loss: 1.4481 - val_binary_accuracy: 0.5233\n","Epoch 15/50\n","36/36 [==============================] - 0s 13ms/step - loss: 1.3696 - binary_accuracy: 0.6522 - val_loss: 1.4350 - val_binary_accuracy: 0.5300\n","Epoch 16/50\n","36/36 [==============================] - 1s 15ms/step - loss: 1.3548 - binary_accuracy: 0.6644 - val_loss: 1.4243 - val_binary_accuracy: 0.5400\n","Epoch 17/50\n","36/36 [==============================] - 1s 14ms/step - loss: 1.3404 - binary_accuracy: 0.6722 - val_loss: 1.4115 - val_binary_accuracy: 0.5500\n","Epoch 18/50\n","36/36 [==============================] - 0s 12ms/step - loss: 1.3266 - binary_accuracy: 0.6956 - val_loss: 1.4058 - val_binary_accuracy: 0.5433\n","Epoch 19/50\n","36/36 [==============================] - 0s 13ms/step - loss: 1.3132 - binary_accuracy: 0.7000 - val_loss: 1.3931 - val_binary_accuracy: 0.5633\n","Epoch 20/50\n","36/36 [==============================] - 0s 13ms/step - loss: 1.3000 - binary_accuracy: 0.7122 - val_loss: 1.3838 - val_binary_accuracy: 0.5633\n","Epoch 21/50\n","36/36 [==============================] - 0s 12ms/step - loss: 1.2872 - binary_accuracy: 0.7100 - val_loss: 1.3727 - val_binary_accuracy: 0.5700\n","Epoch 22/50\n","36/36 [==============================] - 0s 14ms/step - loss: 1.2745 - binary_accuracy: 0.7300 - val_loss: 1.3670 - val_binary_accuracy: 0.5667\n","Epoch 23/50\n","36/36 [==============================] - 1s 15ms/step - loss: 1.2622 - binary_accuracy: 0.7356 - val_loss: 1.3531 - val_binary_accuracy: 0.5800\n","Epoch 24/50\n","36/36 [==============================] - 0s 13ms/step - loss: 1.2505 - binary_accuracy: 0.7500 - val_loss: 1.3487 - val_binary_accuracy: 0.5700\n","Epoch 25/50\n","36/36 [==============================] - 0s 14ms/step - loss: 1.2387 - binary_accuracy: 0.7578 - val_loss: 1.3380 - val_binary_accuracy: 0.5800\n","Epoch 26/50\n","36/36 [==============================] - 1s 14ms/step - loss: 1.2272 - binary_accuracy: 0.7733 - val_loss: 1.3307 - val_binary_accuracy: 0.5767\n","Epoch 27/50\n","36/36 [==============================] - 1s 14ms/step - loss: 1.2159 - binary_accuracy: 0.7711 - val_loss: 1.3204 - val_binary_accuracy: 0.5833\n","Epoch 28/50\n","36/36 [==============================] - 1s 15ms/step - loss: 1.2049 - binary_accuracy: 0.7778 - val_loss: 1.3121 - val_binary_accuracy: 0.5867\n","Epoch 29/50\n","36/36 [==============================] - 0s 14ms/step - loss: 1.1940 - binary_accuracy: 0.7833 - val_loss: 1.3088 - val_binary_accuracy: 0.5800\n","Epoch 30/50\n","36/36 [==============================] - 0s 14ms/step - loss: 1.1832 - binary_accuracy: 0.7867 - val_loss: 1.2979 - val_binary_accuracy: 0.5900\n","Epoch 31/50\n","36/36 [==============================] - 0s 13ms/step - loss: 1.1727 - binary_accuracy: 0.8078 - val_loss: 1.2913 - val_binary_accuracy: 0.5933\n","Epoch 32/50\n","36/36 [==============================] - 0s 13ms/step - loss: 1.1623 - binary_accuracy: 0.8189 - val_loss: 1.2867 - val_binary_accuracy: 0.5900\n","Epoch 33/50\n","36/36 [==============================] - 0s 13ms/step - loss: 1.1523 - binary_accuracy: 0.8189 - val_loss: 1.2709 - val_binary_accuracy: 0.5967\n","Epoch 34/50\n","36/36 [==============================] - 1s 14ms/step - loss: 1.1425 - binary_accuracy: 0.8344 - val_loss: 1.2706 - val_binary_accuracy: 0.5900\n","Epoch 35/50\n","36/36 [==============================] - 1s 20ms/step - loss: 1.1326 - binary_accuracy: 0.8389 - val_loss: 1.2608 - val_binary_accuracy: 0.5900\n","Epoch 36/50\n","36/36 [==============================] - 1s 16ms/step - loss: 1.1228 - binary_accuracy: 0.8478 - val_loss: 1.2568 - val_binary_accuracy: 0.5900\n","Epoch 37/50\n","36/36 [==============================] - 0s 12ms/step - loss: 1.1134 - binary_accuracy: 0.8489 - val_loss: 1.2477 - val_binary_accuracy: 0.5967\n","Epoch 38/50\n","36/36 [==============================] - 0s 12ms/step - loss: 1.1041 - binary_accuracy: 0.8533 - val_loss: 1.2417 - val_binary_accuracy: 0.5967\n","Epoch 39/50\n","36/36 [==============================] - 0s 11ms/step - loss: 1.0950 - binary_accuracy: 0.8544 - val_loss: 1.2339 - val_binary_accuracy: 0.6000\n","Epoch 40/50\n","36/36 [==============================] - 0s 14ms/step - loss: 1.0859 - binary_accuracy: 0.8611 - val_loss: 1.2273 - val_binary_accuracy: 0.6000\n","Epoch 41/50\n","36/36 [==============================] - 1s 16ms/step - loss: 1.0771 - binary_accuracy: 0.8756 - val_loss: 1.2213 - val_binary_accuracy: 0.6033\n","Epoch 42/50\n","36/36 [==============================] - 1s 14ms/step - loss: 1.0682 - binary_accuracy: 0.8744 - val_loss: 1.2095 - val_binary_accuracy: 0.6067\n","Epoch 43/50\n","36/36 [==============================] - 1s 14ms/step - loss: 1.0595 - binary_accuracy: 0.8822 - val_loss: 1.2031 - val_binary_accuracy: 0.6067\n","Epoch 44/50\n","36/36 [==============================] - 1s 17ms/step - loss: 1.0508 - binary_accuracy: 0.8811 - val_loss: 1.2046 - val_binary_accuracy: 0.6033\n","Epoch 45/50\n","36/36 [==============================] - 0s 13ms/step - loss: 1.0426 - binary_accuracy: 0.8867 - val_loss: 1.1964 - val_binary_accuracy: 0.6067\n","Epoch 46/50\n","36/36 [==============================] - 1s 14ms/step - loss: 1.0341 - binary_accuracy: 0.8956 - val_loss: 1.1950 - val_binary_accuracy: 0.6033\n","Epoch 47/50\n","36/36 [==============================] - 1s 14ms/step - loss: 1.0258 - binary_accuracy: 0.8967 - val_loss: 1.1877 - val_binary_accuracy: 0.6067\n","Epoch 48/50\n","36/36 [==============================] - 0s 13ms/step - loss: 1.0177 - binary_accuracy: 0.8956 - val_loss: 1.1675 - val_binary_accuracy: 0.6300\n","Epoch 49/50\n","36/36 [==============================] - 0s 12ms/step - loss: 1.0097 - binary_accuracy: 0.9133 - val_loss: 1.1730 - val_binary_accuracy: 0.6133\n","Epoch 50/50\n","36/36 [==============================] - 0s 11ms/step - loss: 1.0019 - binary_accuracy: 0.9122 - val_loss: 1.1664 - val_binary_accuracy: 0.6100\n","Epoch 1/50\n","36/36 [==============================] - 2s 25ms/step - loss: 1.6305 - binary_accuracy: 0.4822 - val_loss: 1.6225 - val_binary_accuracy: 0.5733\n","Epoch 2/50\n","36/36 [==============================] - 0s 13ms/step - loss: 1.6167 - binary_accuracy: 0.5400 - val_loss: 1.6116 - val_binary_accuracy: 0.5900\n","Epoch 3/50\n","36/36 [==============================] - 1s 14ms/step - loss: 1.6025 - binary_accuracy: 0.6000 - val_loss: 1.5997 - val_binary_accuracy: 0.6167\n","Epoch 4/50\n","36/36 [==============================] - 1s 14ms/step - loss: 1.5867 - binary_accuracy: 0.6756 - val_loss: 1.5869 - val_binary_accuracy: 0.6233\n","Epoch 5/50\n","36/36 [==============================] - 1s 14ms/step - loss: 1.5692 - binary_accuracy: 0.6778 - val_loss: 1.5721 - val_binary_accuracy: 0.6600\n","Epoch 6/50\n","36/36 [==============================] - 1s 17ms/step - loss: 1.5503 - binary_accuracy: 0.7244 - val_loss: 1.5559 - val_binary_accuracy: 0.7167\n","Epoch 7/50\n","36/36 [==============================] - 0s 13ms/step - loss: 1.5301 - binary_accuracy: 0.7900 - val_loss: 1.5391 - val_binary_accuracy: 0.7367\n","Epoch 8/50\n","36/36 [==============================] - 0s 11ms/step - loss: 1.5088 - binary_accuracy: 0.8089 - val_loss: 1.5212 - val_binary_accuracy: 0.7633\n","Epoch 9/50\n","36/36 [==============================] - 0s 12ms/step - loss: 1.4870 - binary_accuracy: 0.8278 - val_loss: 1.5024 - val_binary_accuracy: 0.7967\n","Epoch 10/50\n","36/36 [==============================] - 0s 11ms/step - loss: 1.4646 - binary_accuracy: 0.8411 - val_loss: 1.4836 - val_binary_accuracy: 0.8067\n","Epoch 11/50\n","36/36 [==============================] - 0s 11ms/step - loss: 1.4419 - binary_accuracy: 0.8589 - val_loss: 1.4643 - val_binary_accuracy: 0.8167\n","Epoch 12/50\n","36/36 [==============================] - 0s 11ms/step - loss: 1.4190 - binary_accuracy: 0.8756 - val_loss: 1.4450 - val_binary_accuracy: 0.8300\n","Epoch 13/50\n","36/36 [==============================] - 0s 11ms/step - loss: 1.3963 - binary_accuracy: 0.8889 - val_loss: 1.4260 - val_binary_accuracy: 0.8367\n","Epoch 14/50\n","36/36 [==============================] - 0s 11ms/step - loss: 1.3737 - binary_accuracy: 0.9022 - val_loss: 1.4075 - val_binary_accuracy: 0.8400\n","Epoch 15/50\n","36/36 [==============================] - 0s 12ms/step - loss: 1.3514 - binary_accuracy: 0.9144 - val_loss: 1.3894 - val_binary_accuracy: 0.8467\n","Epoch 16/50\n","36/36 [==============================] - 0s 11ms/step - loss: 1.3295 - binary_accuracy: 0.9244 - val_loss: 1.3709 - val_binary_accuracy: 0.8533\n","Epoch 17/50\n","36/36 [==============================] - 0s 12ms/step - loss: 1.3073 - binary_accuracy: 0.9300 - val_loss: 1.3532 - val_binary_accuracy: 0.8500\n","Epoch 18/50\n","36/36 [==============================] - 0s 11ms/step - loss: 1.2858 - binary_accuracy: 0.9289 - val_loss: 1.3354 - val_binary_accuracy: 0.8667\n","Epoch 19/50\n","36/36 [==============================] - 0s 11ms/step - loss: 1.2648 - binary_accuracy: 0.9389 - val_loss: 1.3181 - val_binary_accuracy: 0.8700\n","Epoch 20/50\n","36/36 [==============================] - 0s 11ms/step - loss: 1.2440 - binary_accuracy: 0.9411 - val_loss: 1.3011 - val_binary_accuracy: 0.8567\n","Epoch 21/50\n","36/36 [==============================] - 0s 11ms/step - loss: 1.2238 - binary_accuracy: 0.9433 - val_loss: 1.2848 - val_binary_accuracy: 0.8600\n","Epoch 22/50\n","36/36 [==============================] - 0s 12ms/step - loss: 1.2038 - binary_accuracy: 0.9444 - val_loss: 1.2689 - val_binary_accuracy: 0.8533\n","Epoch 23/50\n","36/36 [==============================] - 0s 11ms/step - loss: 1.1844 - binary_accuracy: 0.9467 - val_loss: 1.2533 - val_binary_accuracy: 0.8467\n","Epoch 24/50\n","36/36 [==============================] - 0s 11ms/step - loss: 1.1652 - binary_accuracy: 0.9489 - val_loss: 1.2381 - val_binary_accuracy: 0.8500\n","Epoch 25/50\n","36/36 [==============================] - 0s 11ms/step - loss: 1.1465 - binary_accuracy: 0.9489 - val_loss: 1.2231 - val_binary_accuracy: 0.8467\n","Epoch 26/50\n","36/36 [==============================] - 0s 11ms/step - loss: 1.1282 - binary_accuracy: 0.9522 - val_loss: 1.2088 - val_binary_accuracy: 0.8400\n","Epoch 27/50\n","36/36 [==============================] - 0s 12ms/step - loss: 1.1103 - binary_accuracy: 0.9522 - val_loss: 1.1943 - val_binary_accuracy: 0.8567\n","Epoch 28/50\n","36/36 [==============================] - 0s 11ms/step - loss: 1.0930 - binary_accuracy: 0.9511 - val_loss: 1.1807 - val_binary_accuracy: 0.8433\n","Epoch 29/50\n","36/36 [==============================] - 0s 11ms/step - loss: 1.0756 - binary_accuracy: 0.9578 - val_loss: 1.1678 - val_binary_accuracy: 0.8500\n","Epoch 30/50\n","36/36 [==============================] - 0s 12ms/step - loss: 1.0592 - binary_accuracy: 0.9600 - val_loss: 1.1542 - val_binary_accuracy: 0.8433\n","Epoch 31/50\n","36/36 [==============================] - 0s 11ms/step - loss: 1.0430 - binary_accuracy: 0.9589 - val_loss: 1.1413 - val_binary_accuracy: 0.8400\n","Epoch 32/50\n","36/36 [==============================] - 0s 11ms/step - loss: 1.0271 - binary_accuracy: 0.9589 - val_loss: 1.1292 - val_binary_accuracy: 0.8500\n","Epoch 33/50\n","36/36 [==============================] - 0s 11ms/step - loss: 1.0116 - binary_accuracy: 0.9622 - val_loss: 1.1169 - val_binary_accuracy: 0.8433\n","Epoch 34/50\n","36/36 [==============================] - 0s 12ms/step - loss: 0.9967 - binary_accuracy: 0.9622 - val_loss: 1.1054 - val_binary_accuracy: 0.8500\n","Epoch 35/50\n","36/36 [==============================] - 0s 12ms/step - loss: 0.9817 - binary_accuracy: 0.9656 - val_loss: 1.0944 - val_binary_accuracy: 0.8533\n","Epoch 36/50\n","36/36 [==============================] - 0s 11ms/step - loss: 0.9674 - binary_accuracy: 0.9667 - val_loss: 1.0832 - val_binary_accuracy: 0.8533\n","Epoch 37/50\n","36/36 [==============================] - 0s 11ms/step - loss: 0.9533 - binary_accuracy: 0.9656 - val_loss: 1.0727 - val_binary_accuracy: 0.8500\n","Epoch 38/50\n","36/36 [==============================] - 0s 11ms/step - loss: 0.9397 - binary_accuracy: 0.9678 - val_loss: 1.0612 - val_binary_accuracy: 0.8500\n","Epoch 39/50\n","36/36 [==============================] - 0s 11ms/step - loss: 0.9263 - binary_accuracy: 0.9700 - val_loss: 1.0519 - val_binary_accuracy: 0.8567\n","Epoch 40/50\n","36/36 [==============================] - 0s 12ms/step - loss: 0.9133 - binary_accuracy: 0.9689 - val_loss: 1.0409 - val_binary_accuracy: 0.8500\n","Epoch 41/50\n","36/36 [==============================] - 0s 12ms/step - loss: 0.9005 - binary_accuracy: 0.9711 - val_loss: 1.0321 - val_binary_accuracy: 0.8600\n","Epoch 42/50\n","36/36 [==============================] - 0s 12ms/step - loss: 0.8882 - binary_accuracy: 0.9722 - val_loss: 1.0221 - val_binary_accuracy: 0.8533\n","Epoch 43/50\n","36/36 [==============================] - 0s 12ms/step - loss: 0.8761 - binary_accuracy: 0.9733 - val_loss: 1.0121 - val_binary_accuracy: 0.8467\n","Epoch 44/50\n","36/36 [==============================] - 0s 11ms/step - loss: 0.8643 - binary_accuracy: 0.9744 - val_loss: 1.0032 - val_binary_accuracy: 0.8467\n","Epoch 45/50\n","36/36 [==============================] - 0s 11ms/step - loss: 0.8525 - binary_accuracy: 0.9756 - val_loss: 0.9954 - val_binary_accuracy: 0.8567\n","Epoch 46/50\n","36/36 [==============================] - 0s 12ms/step - loss: 0.8415 - binary_accuracy: 0.9744 - val_loss: 0.9860 - val_binary_accuracy: 0.8567\n","Epoch 47/50\n","36/36 [==============================] - 0s 12ms/step - loss: 0.8304 - binary_accuracy: 0.9767 - val_loss: 0.9773 - val_binary_accuracy: 0.8533\n","Epoch 48/50\n","36/36 [==============================] - 0s 12ms/step - loss: 0.8197 - binary_accuracy: 0.9767 - val_loss: 0.9695 - val_binary_accuracy: 0.8567\n","Epoch 49/50\n","36/36 [==============================] - 0s 11ms/step - loss: 0.8090 - binary_accuracy: 0.9789 - val_loss: 0.9607 - val_binary_accuracy: 0.8467\n","Epoch 50/50\n","36/36 [==============================] - 0s 12ms/step - loss: 0.7987 - binary_accuracy: 0.9789 - val_loss: 0.9529 - val_binary_accuracy: 0.8533\n","Epoch 1/50\n","36/36 [==============================] - 1s 16ms/step - loss: 1.6159 - binary_accuracy: 0.5800 - val_loss: 1.6132 - val_binary_accuracy: 0.6233\n","Epoch 2/50\n","36/36 [==============================] - 0s 12ms/step - loss: 1.5931 - binary_accuracy: 0.6744 - val_loss: 1.5943 - val_binary_accuracy: 0.6700\n","Epoch 3/50\n","36/36 [==============================] - 0s 12ms/step - loss: 1.5681 - binary_accuracy: 0.7600 - val_loss: 1.5735 - val_binary_accuracy: 0.7333\n","Epoch 4/50\n","36/36 [==============================] - 0s 12ms/step - loss: 1.5410 - binary_accuracy: 0.8211 - val_loss: 1.5519 - val_binary_accuracy: 0.7733\n","Epoch 5/50\n","36/36 [==============================] - 0s 12ms/step - loss: 1.5123 - binary_accuracy: 0.8533 - val_loss: 1.5302 - val_binary_accuracy: 0.8000\n","Epoch 6/50\n","36/36 [==============================] - 0s 11ms/step - loss: 1.4836 - binary_accuracy: 0.8722 - val_loss: 1.5074 - val_binary_accuracy: 0.8033\n","Epoch 7/50\n","36/36 [==============================] - 0s 12ms/step - loss: 1.4548 - binary_accuracy: 0.8844 - val_loss: 1.4851 - val_binary_accuracy: 0.8133\n","Epoch 8/50\n","36/36 [==============================] - 0s 12ms/step - loss: 1.4264 - binary_accuracy: 0.8944 - val_loss: 1.4639 - val_binary_accuracy: 0.8267\n","Epoch 9/50\n","36/36 [==============================] - 0s 12ms/step - loss: 1.3988 - binary_accuracy: 0.9078 - val_loss: 1.4425 - val_binary_accuracy: 0.8300\n","Epoch 10/50\n","36/36 [==============================] - 0s 12ms/step - loss: 1.3718 - binary_accuracy: 0.9111 - val_loss: 1.4218 - val_binary_accuracy: 0.8367\n","Epoch 11/50\n","36/36 [==============================] - 0s 12ms/step - loss: 1.3459 - binary_accuracy: 0.9167 - val_loss: 1.4018 - val_binary_accuracy: 0.8367\n","Epoch 12/50\n","36/36 [==============================] - 0s 12ms/step - loss: 1.3203 - binary_accuracy: 0.9200 - val_loss: 1.3821 - val_binary_accuracy: 0.8400\n","Epoch 13/50\n","36/36 [==============================] - 0s 12ms/step - loss: 1.2955 - binary_accuracy: 0.9300 - val_loss: 1.3632 - val_binary_accuracy: 0.8367\n","Epoch 14/50\n","36/36 [==============================] - 0s 11ms/step - loss: 1.2716 - binary_accuracy: 0.9333 - val_loss: 1.3455 - val_binary_accuracy: 0.8400\n","Epoch 15/50\n","36/36 [==============================] - 0s 11ms/step - loss: 1.2481 - binary_accuracy: 0.9356 - val_loss: 1.3279 - val_binary_accuracy: 0.8433\n","Epoch 16/50\n","36/36 [==============================] - 0s 12ms/step - loss: 1.2255 - binary_accuracy: 0.9400 - val_loss: 1.3105 - val_binary_accuracy: 0.8400\n","Epoch 17/50\n","36/36 [==============================] - 0s 11ms/step - loss: 1.2035 - binary_accuracy: 0.9422 - val_loss: 1.2931 - val_binary_accuracy: 0.8367\n","Epoch 18/50\n","36/36 [==============================] - 0s 11ms/step - loss: 1.1823 - binary_accuracy: 0.9489 - val_loss: 1.2773 - val_binary_accuracy: 0.8433\n","Epoch 19/50\n","36/36 [==============================] - 0s 11ms/step - loss: 1.1618 - binary_accuracy: 0.9500 - val_loss: 1.2627 - val_binary_accuracy: 0.8433\n","Epoch 20/50\n","36/36 [==============================] - 0s 12ms/step - loss: 1.1417 - binary_accuracy: 0.9456 - val_loss: 1.2486 - val_binary_accuracy: 0.8400\n","Epoch 21/50\n","36/36 [==============================] - 0s 12ms/step - loss: 1.1227 - binary_accuracy: 0.9511 - val_loss: 1.2328 - val_binary_accuracy: 0.8500\n","Epoch 22/50\n","36/36 [==============================] - 0s 13ms/step - loss: 1.1045 - binary_accuracy: 0.9511 - val_loss: 1.2190 - val_binary_accuracy: 0.8500\n","Epoch 23/50\n","36/36 [==============================] - 0s 11ms/step - loss: 1.0866 - binary_accuracy: 0.9533 - val_loss: 1.2058 - val_binary_accuracy: 0.8500\n","Epoch 24/50\n","36/36 [==============================] - 0s 11ms/step - loss: 1.0694 - binary_accuracy: 0.9567 - val_loss: 1.1929 - val_binary_accuracy: 0.8500\n","Epoch 25/50\n","36/36 [==============================] - 0s 12ms/step - loss: 1.0525 - binary_accuracy: 0.9578 - val_loss: 1.1813 - val_binary_accuracy: 0.8567\n","Epoch 26/50\n","36/36 [==============================] - 0s 11ms/step - loss: 1.0367 - binary_accuracy: 0.9578 - val_loss: 1.1712 - val_binary_accuracy: 0.8433\n","Epoch 27/50\n","36/36 [==============================] - 0s 11ms/step - loss: 1.0212 - binary_accuracy: 0.9611 - val_loss: 1.1596 - val_binary_accuracy: 0.8500\n","Epoch 28/50\n","36/36 [==============================] - 0s 12ms/step - loss: 1.0062 - binary_accuracy: 0.9644 - val_loss: 1.1474 - val_binary_accuracy: 0.8567\n","Epoch 29/50\n","36/36 [==============================] - 0s 12ms/step - loss: 0.9916 - binary_accuracy: 0.9656 - val_loss: 1.1365 - val_binary_accuracy: 0.8500\n","Epoch 30/50\n","36/36 [==============================] - 0s 12ms/step - loss: 0.9772 - binary_accuracy: 0.9667 - val_loss: 1.1266 - val_binary_accuracy: 0.8500\n","Epoch 31/50\n","36/36 [==============================] - 0s 11ms/step - loss: 0.9636 - binary_accuracy: 0.9689 - val_loss: 1.1165 - val_binary_accuracy: 0.8500\n","Epoch 32/50\n","36/36 [==============================] - 0s 11ms/step - loss: 0.9500 - binary_accuracy: 0.9667 - val_loss: 1.1078 - val_binary_accuracy: 0.8500\n","Epoch 33/50\n","36/36 [==============================] - 0s 12ms/step - loss: 0.9369 - binary_accuracy: 0.9689 - val_loss: 1.0974 - val_binary_accuracy: 0.8533\n","Epoch 34/50\n","36/36 [==============================] - 0s 11ms/step - loss: 0.9244 - binary_accuracy: 0.9733 - val_loss: 1.0881 - val_binary_accuracy: 0.8500\n","Epoch 35/50\n","36/36 [==============================] - 0s 11ms/step - loss: 0.9119 - binary_accuracy: 0.9700 - val_loss: 1.0784 - val_binary_accuracy: 0.8533\n","Epoch 36/50\n","36/36 [==============================] - 0s 11ms/step - loss: 0.9000 - binary_accuracy: 0.9733 - val_loss: 1.0699 - val_binary_accuracy: 0.8500\n","Epoch 37/50\n","36/36 [==============================] - 0s 12ms/step - loss: 0.8882 - binary_accuracy: 0.9722 - val_loss: 1.0619 - val_binary_accuracy: 0.8533\n","Epoch 38/50\n","36/36 [==============================] - 0s 12ms/step - loss: 0.8767 - binary_accuracy: 0.9756 - val_loss: 1.0552 - val_binary_accuracy: 0.8533\n","Epoch 39/50\n","36/36 [==============================] - 0s 11ms/step - loss: 0.8653 - binary_accuracy: 0.9767 - val_loss: 1.0446 - val_binary_accuracy: 0.8500\n","Epoch 40/50\n","36/36 [==============================] - 0s 12ms/step - loss: 0.8548 - binary_accuracy: 0.9778 - val_loss: 1.0376 - val_binary_accuracy: 0.8567\n","Epoch 41/50\n","36/36 [==============================] - 0s 11ms/step - loss: 0.8438 - binary_accuracy: 0.9778 - val_loss: 1.0308 - val_binary_accuracy: 0.8500\n","Epoch 42/50\n","36/36 [==============================] - 0s 11ms/step - loss: 0.8334 - binary_accuracy: 0.9822 - val_loss: 1.0221 - val_binary_accuracy: 0.8533\n","Epoch 43/50\n","36/36 [==============================] - 0s 12ms/step - loss: 0.8229 - binary_accuracy: 0.9800 - val_loss: 1.0157 - val_binary_accuracy: 0.8500\n","Epoch 44/50\n","36/36 [==============================] - 0s 11ms/step - loss: 0.8130 - binary_accuracy: 0.9811 - val_loss: 1.0075 - val_binary_accuracy: 0.8500\n","Epoch 45/50\n","36/36 [==============================] - 0s 11ms/step - loss: 0.8031 - binary_accuracy: 0.9800 - val_loss: 1.0004 - val_binary_accuracy: 0.8500\n","Epoch 46/50\n","36/36 [==============================] - 0s 13ms/step - loss: 0.7935 - binary_accuracy: 0.9822 - val_loss: 0.9938 - val_binary_accuracy: 0.8500\n","Epoch 47/50\n","36/36 [==============================] - 0s 11ms/step - loss: 0.7837 - binary_accuracy: 0.9833 - val_loss: 0.9864 - val_binary_accuracy: 0.8567\n","Epoch 48/50\n","36/36 [==============================] - 0s 12ms/step - loss: 0.7746 - binary_accuracy: 0.9833 - val_loss: 0.9798 - val_binary_accuracy: 0.8567\n","Epoch 49/50\n","36/36 [==============================] - 0s 12ms/step - loss: 0.7653 - binary_accuracy: 0.9844 - val_loss: 0.9736 - val_binary_accuracy: 0.8567\n","Epoch 50/50\n","36/36 [==============================] - 0s 12ms/step - loss: 0.7564 - binary_accuracy: 0.9844 - val_loss: 0.9683 - val_binary_accuracy: 0.8433\n"]}]},{"cell_type":"code","source":["# Plot validation performance. \n","plt.plot(average_loss_history,c='r')\n","plt.plot(average_acc_history,c=\"r\",linestyle=\"dashed\")\n","plt.plot(average_val_loss_history,c='b')\n","plt.plot(average_val_acc_history,c='b',linestyle=\"dashed\")\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Loss\")\n","plt.legend(['Training Loss','Training Accuracy','Validation Loss','Validation Accuracy'])\n","plt.show()"],"metadata":{"id":"rRmEzDzHVeh9"},"execution_count":null,"outputs":[]}]}