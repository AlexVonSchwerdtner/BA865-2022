{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Functional API.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNO1tuoRRu7Nr+T90vkQ4eK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#**Working with the Functional vs. Sequential API (or Both!)**"],"metadata":{"id":"wqPCmdmg3m4T"}},{"cell_type":"markdown","source":["I'm going to load the Boston Housing dataset again, just for demonstration purposes."],"metadata":{"id":"t0nbIEv43ulp"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZJilNqN43jzW"},"outputs":[],"source":["from tensorflow.keras.datasets import boston_housing\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","import numpy as np\n","\n","(train_data, train_targets), (test_data, test_targets) = (\n","    boston_housing.load_data())"]},{"cell_type":"markdown","source":["#*Simple, Feed-forward Topology*"],"metadata":{"id":"T4jLygRF-o3O"}},{"cell_type":"markdown","source":["Recall that we could specify a Sequential model as follows..."],"metadata":{"id":"rFTADU3R36-m"}},{"cell_type":"code","source":["# Start the model... note that I can 'name' the model as part of this.\n","model = keras.Sequential(name=\"Boston Housing Model\")\n","\n","# Add some layers... I can name those too. \n","# The Sequential API approach forces me to full connect everything I add to the last layer of what I've done so far. \n","# It only alows a single pipe from input to output, of variable width. \n","model.add(layers.Input(13, name=\"inputs\"))\n","model.add(layers.Dense(8, activation=\"relu\",name=\"first_hidden\"))\n","model.add(layers.Dense(8, activation=\"relu\",name=\"second_hidden\"))\n","model.add(layers.Dense(1,name=\"output\"))\n","\n","# Compile the model.\n","model.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[\"mae\"])\n","\n","# Draw a picture of the model... \n","keras.utils.plot_model(model,show_shapes=True)"],"metadata":{"id":"TlkNXwET396k"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can specify the exact same model using the Functional API, as follows... "],"metadata":{"id":"DtvjHgOj6lKy"}},{"cell_type":"code","source":["# I define my input layer, with shape = number of predictors in the data.\n","inputs = keras.layers.Input(shape=(train_data.shape[1],),name=\"inputs\")\n","\n","# I then define the first hidden layer, which is connected to the prior input layer.\n","# I can connect the input layer to this, and I could connect some other layer... \n","first_hidden = keras.layers.Dense(8, activation=\"relu\", name=\"first_hidden\")(inputs)\n","second_hidden = keras.layers.Dense(8, activation=\"relu\", name=\"second_hidden\")(first_hidden)\n","\n","# I then define my output layer, which connects to the hidden layer.\n","output = keras.layers.Dense(1, name=\"output\")(second_hidden)\n","\n","# Finally, I feed the defined layer structure into a Keras model. \n","model = keras.Model(name=\"Bouston Housing\",inputs=[inputs],outputs=[output])\n","\n","# Draw a picture of the model. \n","keras.utils.plot_model(model,show_shapes=True)"],"metadata":{"id":"Mv5QchpB6oto"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#*More Complex Toplogy with Functional API*"],"metadata":{"id":"4V3pwY6r-tNp"}},{"cell_type":"markdown","source":["Now we can use the functional API to make almost any topology we want, mixing and matching layers, and creating whatever branching logic we like..."],"metadata":{"id":"qn65XxfZ-VJs"}},{"cell_type":"code","source":["# I define my input layer, with shape = number of predictors in the data.\n","inputs = keras.layers.Input(shape=(train_data.shape[1],),name=\"inputs\")\n","\n","# I then define the first hidden layer, which is connected to the prior input layer.\n","# I can connect the input layer to this, and I could connect some other layer... \n","one_one_hidden = keras.layers.Dense(8, activation=\"relu\", name=\"branch_one_one\")(inputs)\n","one_two_hidden = keras.layers.Dense(8, activation=\"relu\", name=\"branch_one_two\")(one_one_hidden)\n","\n","# I then define my output layer, which takes input from the prior set of hidden layers, in addition to taking the inputs directly as well.\n","merge = keras.layers.Concatenate()([one_two_hidden,inputs])\n","output = keras.layers.Dense(1, name=\"output\")(merge)\n","\n","# Finally, I feed the defined layer structure into a Keras model. \n","model = keras.Model(name=\"Bouston Housing\",inputs=[inputs],outputs=[output])\n","\n","# Draw a picture of the model. \n","keras.utils.plot_model(model,show_shapes=True)"],"metadata":{"id":"YpzwaBOc-x-C"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["For kicks, let's see how this model performs on the Boston Housing data. First, let's wrap the model build code inside a build function. Note that I'm adding a batch norm layer to whiten the inputs. "],"metadata":{"id":"VQJfoPfEZvo1"}},{"cell_type":"code","source":["def build_model(n_predictors=13):\n","    # I define my input layer, with shape = number of predictors in the data.\n","    inputs = keras.layers.Input(shape=(n_predictors,),name=\"inputs\")\n","    norm = keras.layers.BatchNormalization(center=True)(inputs)\n","\n","    # I then define the first hidden layer, which is connected to the prior input layer.\n","    # I can connect the input layer to this, and I could connect some other layer... \n","    one_one_hidden = keras.layers.Dense(8, activation=\"relu\", name=\"branch_one_one\")(norm)\n","    one_two_hidden = keras.layers.Dense(8, activation=\"relu\", name=\"branch_one_two\")(one_one_hidden)\n","\n","    # I then define my output layer, which takes input from the prior set of hidden layers, in addition to taking the inputs directly as well.\n","    merge = keras.layers.Concatenate()([one_two_hidden,norm])\n","    output = keras.layers.Dense(1, name=\"output\")(merge)\n","\n","    # Finally, I feed the defined layer structure into a Keras model. \n","    model = keras.Model(inputs=[inputs],outputs=[output])\n","\n","    model.compile(optimizer=\"rmsprop\",loss=\"mae\",metrics=['mae'])\n","    return model"],"metadata":{"id":"2yB0BldEZ7hy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["And, let's do the same cross-validation we did last time..."],"metadata":{"id":"n3-tQ0u1aY0j"}},{"cell_type":"code","source":["k = 3 \n","num_val_samples = len(train_data) // k # floor division (i.e., round down to nearest integer.)\n","num_epochs = 150\n","batch_size = 25\n","all_mae_histories = []  \n","\n","print(\"In total, we have\",len(train_data),\"training observations.\")\n","print(\"With a k of\",k,\"we have\",num_val_samples,\"observations per fold.\\n\")\n","\n","for i in range(k): # the folds are going to be indexed 0 through 3 if k = 4\n","    print(\"Processing fold #:\",i)\n","    # if I slice past the end of the array, it just gives me what it can find! No errors.\n","    # This is important here, because the last fold won't produce an error, despite our slice going well beyond the end of the array.\n","    print(\"Validation data includes observations\",i*num_val_samples,\"through\",(i+1)*num_val_samples-1) # minus 1 because a slice is up to and not including the second index.\n","    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples] \n","    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n","    print(\"Training data includes observations 0 through\",i*num_val_samples-1,\"joined with observations\",(i+1)*num_val_samples,\"through the final observation.\\n\")\n","    partial_train_data = np.concatenate(\n","        [train_data[:i * num_val_samples],\n","         train_data[(i + 1) * num_val_samples:]],\n","        axis=0)\n","    partial_train_targets = np.concatenate(\n","        [train_targets[:i * num_val_samples],\n","         train_targets[(i + 1) * num_val_samples:]],\n","        axis=0)\n","    model = build_model()\n","    history = model.fit(partial_train_data, partial_train_targets,\n","                        validation_data=(val_data, val_targets),\n","                        epochs=num_epochs, batch_size=batch_size, verbose=0)\n","    mae_history = history.history['val_mae']\n","    all_mae_histories.append(mae_history)\n","\n","average_mae_history = [np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)]"],"metadata":{"id":"xAQhuHM-Z4n2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["And, finally, plot the loss over the training process... "],"metadata":{"id":"n0P7ECDmbf43"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","# The final cross-fold average MAE in our training was about $2,900.\n","print(average_mae_history[-1])\n","\n","plt.plot(average_mae_history)\n","plt.ylabel('Validation MAE')\n","plt.xlabel('Epoch')\n","plt.title('Validation Loss Over Training')\n","plt.show()"],"metadata":{"id":"xI82wxLEbtXI"},"execution_count":null,"outputs":[]}]}